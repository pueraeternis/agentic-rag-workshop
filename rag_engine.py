from pathlib import Path

from llama_index.core import (
    Settings,
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
    load_index_from_storage,
)
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
# –ò—Å–ø–æ–ª—å–∑—É–µ–º Path –¥–ª—è –∫—Ä–æ—Å—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏
PERSIST_DIR = Path("./index_store")
DATA_DIR = Path("./data")

# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π (Ollama)
print("‚öôÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Ollama...")

# –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
Settings.llm = Ollama(
    model="qwen3:8b",
    base_url="http://localhost:11434",
    request_timeout=300.0,
    temperature=0,
)

# –≠–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª—å
Settings.embed_model = OllamaEmbedding(
    model_name="nomic-embed-text",
    base_url="http://localhost:11434",
)


def get_index():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å.
    Pattern: Checkpointer (–ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ü–∏—è)
    """
    # Refactor: os.path.exists -> Path.exists()
    if not PERSIST_DIR.exists():
        print(f"üìÇ –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {PERSIST_DIR}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π...")

        # LlamaIndex –æ—Ç–ª–∏—á–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã Path
        documents = SimpleDirectoryReader(input_dir=DATA_DIR).load_data()
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        index = VectorStoreIndex.from_documents(documents)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
        index.storage_context.persist(persist_dir=str(PERSIST_DIR))
        print("üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
    else:
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏–∑ {PERSIST_DIR}...")
        storage_context = StorageContext.from_defaults(persist_dir=str(PERSIST_DIR))
        index = load_index_from_storage(storage_context)

    return index


def get_rag_tool_function():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Å–∫–æ—Ä–º–∏—Ç—å –ê–≥–µ–Ω—Ç—É.
    """
    index = get_index()
    # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (top_k=3 - –±–µ—Ä–µ–º 3 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∫—É—Å–∫–∞)
    query_engine = index.as_query_engine(similarity_top_k=3)

    def search_knowledge_base(query: str) -> str:
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏."""
        response = query_engine.query(query)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ + –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        return str(response)

    return search_knowledge_base


# –ë–ª–æ–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
if __name__ == "__main__":
    tool = get_rag_tool_function()
    print("\n--- –¢–ï–°–¢ –ü–û–ò–°–ö–ê ---")
    res = tool("–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É 429?")
    print(res)
