from pathlib import Path

from llama_index.core import (
    Settings,
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
    load_index_from_storage,
)
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PERSIST_DIR = Path("./index_store")
DATA_DIR = Path("./data")

# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π (Ollama)
print("‚öôÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Ollama...")

# –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
Settings.llm = Ollama(
    model="qwen3:8b",
    base_url="http://localhost:11434",
    request_timeout=300.0,
    temperature=0,
)

# –≠–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª—å
Settings.embed_model = OllamaEmbedding(
    model_name="nomic-embed-text",
    base_url="http://localhost:11434",
)


def get_index():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å.
    Pattern: Checkpointer (–ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ü–∏—è)
    """
    if not PERSIST_DIR.exists():
        print(f"üìÇ –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {PERSIST_DIR}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π...")

        documents = SimpleDirectoryReader(input_dir=DATA_DIR).load_data()
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        index = VectorStoreIndex.from_documents(documents)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
        index.storage_context.persist(persist_dir=str(PERSIST_DIR))
        print("üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
    else:
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏–∑ {PERSIST_DIR}...")
        storage_context = StorageContext.from_defaults(persist_dir=str(PERSIST_DIR))
        index = load_index_from_storage(storage_context)

    return index


def get_rag_tool_function():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.
    """
    index = get_index()

    retriever = index.as_retriever(similarity_top_k=3)

    def search_knowledge_base(query: str) -> str:
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏."""
        # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ (Nodes)
        nodes = retriever.retrieve(query)

        # 2. –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —É–∑–ª–æ–≤ –≤—Ä—É—á–Ω—É—é
        context_str = "\n\n".join(
            [f"--- –ò—Å—Ç–æ—á–Ω–∏–∫ {i + 1} ---\n{node.get_content()}" for i, node in enumerate(nodes)],
        )

        return context_str

    return search_knowledge_base


# –ë–ª–æ–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
if __name__ == "__main__":
    tool = get_rag_tool_function()
    print("\n--- –¢–ï–°–¢ –ü–û–ò–°–ö–ê ---")
    res = tool("–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É 429?")
    print(res)
